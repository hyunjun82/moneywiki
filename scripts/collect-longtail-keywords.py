#!/usr/bin/env python3
"""
ìŠˆí¼ ë¡±í…Œì¼ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸° v1.0
- ë„¤ì´ë²„, ë‹¤ìŒ, êµ¬ê¸€, ë¹™ì—ì„œ ì—°ê´€ê²€ìƒ‰ì–´ ìˆ˜ì§‘
- ê¼¬ë¦¬ë¬¼ê¸°: ì—°ê´€ê²€ìƒ‰ì–´ì˜ ì—°ê´€ê²€ìƒ‰ì–´ ì¬ê·€ ìˆ˜ì§‘
- 2024ë…„ ì´ì „ êµ¬ë²„ì „ í‚¤ì›Œë“œ ìë™ í•„í„°ë§
- í—ˆë¸Œ â†’ ìŠ¤í¬í¬ â†’ ì„œë¸ŒìŠ¤í¬í¬ â†’ ìŠˆí¼ë¡±í…Œì¼ êµ¬ì¡° ìë™ ë¶„ë¥˜

ì‚¬ìš©ë²•:
  python collect-longtail-keywords.py í‡´ì§ê¸ˆ
  python collect-longtail-keywords.py ì—°ë§ì •ì‚° --depth 3
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
import sys
import os
from urllib.parse import quote
from datetime import datetime

# === ì„¤ì • ===
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), '.claude', 'keywords')

# === ì—°ë„ í•„í„°ë§ ===
def is_current_keyword(keyword):
    """2025ë…„ ë¯¸ë§Œ 'ì² ì§€ë‚œ í‚¤ì›Œë“œ' íŒë…ê¸°"""
    # 1. í‚¤ì›Œë“œì—ì„œ '20xx' í˜•íƒœì˜ ìˆ«ì(ì—°ë„)ë¥¼ ëª¨ë‘ ì°¾ìŒ
    years = re.findall(r"20(\d{2})", keyword)

    for year in years:
        full_year = int(f"20{year}")
        # 2. ë°œê²¬ëœ ì—°ë„ê°€ 2025ë…„ë³´ë‹¤ ì‘ìœ¼ë©´ (ì˜ˆ: 2024, 2023...) â†’ íƒˆë½(False)
        if full_year < 2025:
            return False

    # 3. '24ë…„', '23ë…„' ê°™ì€ ì•½ì–´ íŒ¨í„´ë„ ê²€ì‚¬
    short_years = re.findall(r"(\d{2})ë…„", keyword)
    for sy in short_years:
        # 10-24 ì‚¬ì´ì˜ ìˆ«ìê°€ 'ë…„' ì•ì— ë¶™ìœ¼ë©´ (ì˜ˆ: 24ë…„ ì—°ë§ì •ì‚°) â†’ íƒˆë½
        if 10 <= int(sy) <= 24:
            return False

    return True  # ì—°ë„ê°€ ì—†ê±°ë‚˜, 2025ë…„ ì´ìƒì´ë©´ í†µê³¼


# === í‚¤ì›Œë“œ ê´€ë ¨ì„± í•„í„° ===
def is_relevant_keyword(keyword, seed_keyword):
    """ì‹œë“œ í‚¤ì›Œë“œì™€ ê´€ë ¨ ì—†ëŠ” ë…¸ì´ì¦ˆ í•„í„°ë§"""
    keyword_lower = keyword.lower().strip()

    # 1. ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í‚¤ì›Œë“œ ì œì™¸
    if len(keyword) < 2 or len(keyword) > 50:
        return False

    # 2. UI ìš”ì†Œ/ë„¤ë¹„ê²Œì´ì…˜ ì œì™¸
    ui_patterns = [
        r"^ì—´ê¸°$", r"^ë‹«ê¸°$", r"^ë”ë³´ê¸°$", r"^ë„ì›€ë§$", r"^ê²€ìƒ‰$",
        r"ê²€ìƒ‰ì–´.*ê¸°ëŠ¥", r"ê¸°ëŠ¥ ë‹«ê¸°", r"ê¸°ëŠ¥ ì—´ê¸°",
        r"^ì‹ ê³ $", r"^ê³µìœ $", r"^ì €ì¥$", r"^ì„¤ì •$",
        r"html.*ì—´ê¸°", r"íŒŒì¼.*ì—´ê¸°"
    ]
    for pattern in ui_patterns:
        if re.search(pattern, keyword, re.IGNORECASE):
            return False

    # 3. ê¹¨ì§„ ë¬¸ì/íŠ¹ìˆ˜ë¬¸ì ì œì™¸
    if re.search(r"[é¹¹èµ½é—è·ºæ¹®é½ªç§·å¤”é®è‚­æ¥·ç‹å²†æ¨‘å„„]", keyword):
        return False
    if re.search(r"[\u4e00-\u9fff]{5,}", keyword):  # ì—°ì† í•œì 5ê°œ ì´ìƒ
        return False

    # 4. ëª…í™•íˆ ë¬´ê´€í•œ ì£¼ì œ íŒ¨í„´
    noise_patterns = [
        r"ê²Œì„", r"ì˜í™”", r"ë“œë¼ë§ˆ", r"ë…¸ë˜", r"ìŒì•…",
        r"ìš”ë¦¬", r"ë ˆì‹œí”¼", r"ë§›ì§‘", r"ì—¬í–‰",
        r"ì¶•êµ¬", r"ì•¼êµ¬", r"ë†êµ¬", r"ìŠ¤í¬ì¸ ",
        r"ê°€ì—°ì„±", r"ëˆ„ì¶œì‚¬ê³ ", r"ê°€ìŠ¤.*ëˆ„ì¶œ",
        r"kijul", r"ê°œê±¸ìŠ¤ëŸ½ë‹¤"
    ]

    for pattern in noise_patterns:
        if re.search(pattern, keyword, re.IGNORECASE) and not re.search(pattern, seed_keyword, re.IGNORECASE):
            return False

    # 5. ì‹œë“œ í‚¤ì›Œë“œì™€ ì „í˜€ ê´€ë ¨ ì—†ëŠ”ì§€ ì²´í¬ (í•œê¸€ ê¸°ì¤€)
    # ì‹œë“œì˜ í•µì‹¬ ë‹¨ì–´ê°€ í‚¤ì›Œë“œì— í¬í•¨ë˜ì–´ì•¼ í•¨
    seed_core = re.sub(r'[^\wê°€-í£]', '', seed_keyword)
    if len(seed_core) >= 2:
        # ì‹œë“œ í‚¤ì›Œë“œì˜ ì²« ê¸€ìë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
        if seed_core[0] not in keyword and seed_keyword not in keyword:
            # ì™„ì „íˆ ë¬´ê´€í•´ ë³´ì´ë©´ ì¶”ê°€ ê²€ì‚¬
            related_terms = get_related_terms(seed_keyword)
            if not any(term in keyword for term in related_terms):
                return False

    return True


def get_related_terms(seed_keyword):
    """ì‹œë“œ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìš©ì–´ ëª©ë¡"""
    # ì‹œë“œë³„ ê´€ë ¨ ìš©ì–´ ë§¤í•‘
    related_map = {
        "í‡´ì§ê¸ˆ": ["í‡´ì§", "í‡´ì‚¬", "ì´ì§", "ê·¼ë¡œ", "ê¸‰ì—¬", "ì„ê¸ˆ", "ê³ ìš©", "ë…¸ë™", "ì—°ê¸ˆ", "IRP", "DC", "DB"],
        "ì—°ë§ì •ì‚°": ["ì†Œë“", "ê³µì œ", "ì„¸ê¸ˆ", "ì„¸ì•¡", "ì—°ë´‰", "ê¸‰ì—¬", "ì›ì²œì§•ìˆ˜"],
        "ì‹¤ì—…ê¸‰ì—¬": ["ê³ ìš©ë³´í—˜", "ì‹¤ì—…", "êµ¬ì§", "ì´ì§", "í‡´ì‚¬"],
    }

    # ê¸°ë³¸ ê´€ë ¨ ìš©ì–´
    base_terms = seed_keyword.split() + [seed_keyword]

    # ë§¤í•‘ëœ ê´€ë ¨ ìš©ì–´ ì¶”ê°€
    for key, terms in related_map.items():
        if key in seed_keyword:
            base_terms.extend(terms)

    return base_terms


# === ë„¤ì´ë²„ ìˆ˜ì§‘ ===
def get_naver(keyword):
    """ë„¤ì´ë²„ ì—°ê´€ê²€ìƒ‰ì–´ + ìë™ì™„ì„± ìˆ˜ì§‘"""
    results = set()
    try:
        # ì—°ê´€ê²€ìƒ‰ì–´
        url = f"https://search.naver.com/search.naver?where=nexearch&query={quote(keyword)}"
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')

        # ì—°ê´€ê²€ìƒ‰ì–´ ì…€ë ‰í„°ë“¤ (2025ë…„ ì—…ë°ì´íŠ¸)
        selectors = [
            ".lst_related_srch .tit",
            "._related_srch .tit",
            ".api_txt_lines",
            ".related_srch .tit",
            "[class*='related'] a",
            ".keyword_list a",
            ".relate_srch a"
        ]
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                txt = item.get_text().strip()
                if txt and is_current_keyword(txt):
                    results.add(txt)

        # ìë™ì™„ì„± API (ì—¬ëŸ¬ ì—”ë“œí¬ì¸íŠ¸ ì‹œë„)
        ac_urls = [
            f"https://ac.search.naver.com/nx/ac?q={quote(keyword)}&con=1&r_format=json&st=100",
            f"https://mac.search.naver.com/mobile/ac?q={quote(keyword)}&st=100&r_format=json",
        ]

        for url_ac in ac_urls:
            try:
                res = requests.get(url_ac, headers=headers, timeout=5)
                if res.text.strip():
                    data = json.loads(res.text)
                    items_data = data.get('items', []) or data.get('suggestions', [])
                    for group in items_data:
                        if isinstance(group, list):
                            for item in group:
                                if isinstance(item, list) and item:
                                    txt = item[0] if isinstance(item[0], str) else str(item[0])
                                    if txt and is_current_keyword(txt):
                                        results.add(txt)
                                elif isinstance(item, str) and is_current_keyword(item):
                                    results.add(item)
                        elif isinstance(group, str) and is_current_keyword(group):
                            results.add(group)
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except:
                continue

    except Exception as e:
        print(f"  [ë„¤ì´ë²„ ì˜¤ë¥˜] {e}")

    return results


# === ë‹¤ìŒ ìˆ˜ì§‘ ===
def get_daum(keyword):
    """ë‹¤ìŒ ì—°ê´€ê²€ìƒ‰ì–´ ìˆ˜ì§‘"""
    results = set()
    try:
        url = f"https://search.daum.net/search?w=tot&q={quote(keyword)}"
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')

        # ì—°ê´€ê²€ìƒ‰ì–´ ì…€ë ‰í„°ë“¤ (2025ë…„ ì—…ë°ì´íŠ¸)
        selectors = [
            ".link_relate",
            ".txt_relate",
            ".relate_keyword a",
            ".suggest_keyword a",
            "[class*='relate'] a",
            ".c-relate-keyword a",
            ".keyword-list a"
        ]

        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                txt = item.get_text().strip()
                if txt and is_current_keyword(txt):
                    results.add(txt)

        # ë‹¤ìŒ ìë™ì™„ì„± API ì‹œë„
        try:
            ac_url = f"https://suggest.search.daum.net/sushi/ac/get?q={quote(keyword)}"
            res_ac = requests.get(ac_url, headers=headers, timeout=5)
            if res_ac.text.strip():
                data = json.loads(res_ac.text)
                suggestions = data.get('items', []) or data.get('results', [])
                for item in suggestions:
                    if isinstance(item, dict):
                        txt = item.get('keyword', '') or item.get('text', '')
                    else:
                        txt = str(item)
                    if txt and is_current_keyword(txt):
                        results.add(txt)
        except:
            pass

    except Exception as e:
        print(f"  [ë‹¤ìŒ ì˜¤ë¥˜] {e}")

    return results


# === êµ¬ê¸€ ìˆ˜ì§‘ ===
def get_google(keyword):
    """êµ¬ê¸€ ìë™ì™„ì„± ìˆ˜ì§‘"""
    results = set()
    try:
        # êµ¬ê¸€ ìë™ì™„ì„± API
        url = f"https://suggestqueries.google.com/complete/search?client=firefox&q={quote(keyword)}"
        res = requests.get(url, headers=headers, timeout=5)
        data = json.loads(res.text)

        if len(data) > 1 and isinstance(data[1], list):
            for item in data[1]:
                if item and is_current_keyword(item):
                    results.add(item)
    except Exception as e:
        print(f"  [êµ¬ê¸€ ì˜¤ë¥˜] {e}")

    return results


# === ë¹™ ìˆ˜ì§‘ ===
def get_bing(keyword):
    """ë¹™ ìë™ì™„ì„± ìˆ˜ì§‘"""
    results = set()
    try:
        url = f"https://api.bing.com/osjson.aspx?query={quote(keyword)}"
        res = requests.get(url, headers=headers, timeout=5)
        data = json.loads(res.text)

        if len(data) > 1 and isinstance(data[1], list):
            for item in data[1]:
                if item and is_current_keyword(item):
                    results.add(item)
    except Exception as e:
        print(f"  [ë¹™ ì˜¤ë¥˜] {e}")

    return results


# === ì „ì—­ ì‹œë“œ í‚¤ì›Œë“œ (ë…¸ì´ì¦ˆ í•„í„°ìš©) ===
SEED_KEYWORD = ""


# === 4ëŒ€ í¬í„¸ í†µí•© ìˆ˜ì§‘ ===
def collect_all_portals(keyword):
    """4ëŒ€ í¬í„¸ì—ì„œ í‚¤ì›Œë“œ ìˆ˜ì§‘"""
    global SEED_KEYWORD
    all_results = set()

    print(f"  ğŸ“ '{keyword}' ìˆ˜ì§‘ ì¤‘...")

    # ë„¤ì´ë²„
    naver = get_naver(keyword)
    all_results.update(naver)
    print(f"    - ë„¤ì´ë²„: {len(naver)}ê°œ")

    time.sleep(random.uniform(0.2, 0.5))

    # ë‹¤ìŒ
    daum = get_daum(keyword)
    all_results.update(daum)
    print(f"    - ë‹¤ìŒ: {len(daum)}ê°œ")

    time.sleep(random.uniform(0.2, 0.5))

    # êµ¬ê¸€
    google = get_google(keyword)
    all_results.update(google)
    print(f"    - êµ¬ê¸€: {len(google)}ê°œ")

    time.sleep(random.uniform(0.2, 0.5))

    # ë¹™
    bing = get_bing(keyword)
    all_results.update(bing)
    print(f"    - ë¹™: {len(bing)}ê°œ")

    # ë…¸ì´ì¦ˆ í•„í„°ë§ (ì‹œë“œ í‚¤ì›Œë“œì™€ ë¬´ê´€í•œ í‚¤ì›Œë“œ ì œê±°)
    if SEED_KEYWORD:
        filtered = {kw for kw in all_results if is_relevant_keyword(kw, SEED_KEYWORD)}
        removed = len(all_results) - len(filtered)
        if removed > 0:
            print(f"    - ë…¸ì´ì¦ˆ ì œê±°: {removed}ê°œ")
        return filtered

    return all_results


# === ê¼¬ë¦¬ë¬¼ê¸° (ì¬ê·€ í™•ì¥) ===
def collect_with_tail_biting(seed_keyword, max_depth=2, max_expand=15):
    """
    ê¼¬ë¦¬ë¬¼ê¸°: ì—°ê´€ê²€ìƒ‰ì–´ì˜ ì—°ê´€ê²€ìƒ‰ì–´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘

    Args:
        seed_keyword: ì‹œì‘ í‚¤ì›Œë“œ (í—ˆë¸Œ)
        max_depth: ìµœëŒ€ í™•ì¥ ê¹Šì´ (1=ìŠ¤í¬í¬, 2=ì„œë¸ŒìŠ¤í¬í¬, 3=ìŠˆí¼ë¡±í…Œì¼)
        max_expand: ê° ë ˆë²¨ì—ì„œ í™•ì¥í•  ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
    """
    global SEED_KEYWORD
    SEED_KEYWORD = seed_keyword  # ë…¸ì´ì¦ˆ í•„í„°ìš© ì‹œë“œ ì„¤ì •
    all_keywords = {seed_keyword}  # ì¤‘ë³µ ë°©ì§€ìš© ì „ì²´ ì§‘í•©
    level_keywords = {0: {seed_keyword}}  # ë ˆë²¨ë³„ í‚¤ì›Œë“œ

    print(f"\nğŸ”¥ [1ë‹¨ê³„] '{seed_keyword}' 1ì°¨ ìˆ˜ì§‘ ì¤‘...")

    # 1ë‹¨ê³„: ì‹œë“œ í‚¤ì›Œë“œì—ì„œ ìˆ˜ì§‘
    level1 = collect_all_portals(seed_keyword)
    level1 = level1 - all_keywords  # ì¤‘ë³µ ì œê±°
    all_keywords.update(level1)
    level_keywords[1] = level1

    print(f"\nâœ… 1ì°¨ ìˆ˜ì§‘: {len(level1)}ê°œ")

    # 2ë‹¨ê³„ ì´ìƒ: ê¼¬ë¦¬ë¬¼ê¸°
    for depth in range(2, max_depth + 1):
        print(f"\nğŸ [{depth}ë‹¨ê³„] ë¡±í…Œì¼ í™•ì¥ ì¤‘ (ê¹Šì´ {depth})...")

        # ì´ì „ ë ˆë²¨ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œë§Œ í™•ì¥
        prev_level = list(level_keywords.get(depth - 1, set()))[:max_expand]
        current_level = set()

        for i, kw in enumerate(prev_level):
            print(f"  [{i+1}/{len(prev_level)}] í™•ì¥: '{kw}'")
            new_keywords = collect_all_portals(kw)
            new_keywords = new_keywords - all_keywords  # ì¤‘ë³µ ì œê±°
            current_level.update(new_keywords)
            all_keywords.update(new_keywords)
            time.sleep(random.uniform(0.3, 0.7))

        level_keywords[depth] = current_level
        print(f"\nâœ… {depth}ì°¨ ìˆ˜ì§‘: {len(current_level)}ê°œ (ì‹ ê·œ)")

    return all_keywords, level_keywords


# === í—ˆë¸Œ-ìŠ¤í¬í¬ êµ¬ì¡° ë¶„ë¥˜ ===
def classify_structure(seed_keyword, level_keywords):
    """
    í—ˆë¸Œ â†’ ìŠ¤í¬í¬ â†’ ì„œë¸ŒìŠ¤í¬í¬ â†’ ìŠˆí¼ë¡±í…Œì¼ êµ¬ì¡° ìë™ ë¶„ë¥˜
    """
    structure = {
        "í—ˆë¸Œ": seed_keyword,
        "ìŠ¤í¬í¬": [],        # 1ë‹¨ê³„ (ë©”ì¸ ì—°ê´€)
        "ì„œë¸ŒìŠ¤í¬í¬": [],    # 2ë‹¨ê³„ (ì„¸ë¶€ ì—°ê´€)
        "ìŠˆí¼ë¡±í…Œì¼": []     # 3ë‹¨ê³„+ (ì´ˆë¡±í…Œì¼)
    }

    # 1ë‹¨ê³„ = ìŠ¤í¬í¬
    if 1 in level_keywords:
        structure["ìŠ¤í¬í¬"] = sorted(list(level_keywords[1]))

    # 2ë‹¨ê³„ = ì„œë¸ŒìŠ¤í¬í¬
    if 2 in level_keywords:
        structure["ì„œë¸ŒìŠ¤í¬í¬"] = sorted(list(level_keywords[2]))

    # 3ë‹¨ê³„+ = ìŠˆí¼ë¡±í…Œì¼
    super_longtail = set()
    for depth in range(3, max(level_keywords.keys()) + 1):
        if depth in level_keywords:
            super_longtail.update(level_keywords[depth])
    structure["ìŠˆí¼ë¡±í…Œì¼"] = sorted(list(super_longtail))

    return structure


# === íŒŒì¼ ìŠ¬ëŸ¬ê·¸ ë³€í™˜ ===
def to_slug(keyword):
    """í‚¤ì›Œë“œë¥¼ íŒŒì¼ëª…ìš© ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜"""
    # ê³µë°± â†’ í•˜ì´í”ˆ
    slug = keyword.replace(" ", "-")
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    slug = re.sub(r'[^\wê°€-í£-]', '', slug)
    return slug


# === ê²°ê³¼ ì €ì¥ ===
def save_results(seed_keyword, all_keywords, structure):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    slug = to_slug(seed_keyword)
    output_path = os.path.join(OUTPUT_DIR, f"{slug}.json")

    result = {
        "í—ˆë¸Œ": seed_keyword,
        "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "ì´_í‚¤ì›Œë“œìˆ˜": len(all_keywords),
        "êµ¬ì¡°": {
            "í—ˆë¸Œ": structure["í—ˆë¸Œ"],
            "ìŠ¤í¬í¬_ê°œìˆ˜": len(structure["ìŠ¤í¬í¬"]),
            "ì„œë¸ŒìŠ¤í¬í¬_ê°œìˆ˜": len(structure["ì„œë¸ŒìŠ¤í¬í¬"]),
            "ìŠˆí¼ë¡±í…Œì¼_ê°œìˆ˜": len(structure["ìŠˆí¼ë¡±í…Œì¼"])
        },
        "ê¸€ì‘ì„±_ìš°ì„ ìˆœìœ„": {
            "1ë‹¨ê³„_ìŠ¤í¬í¬": structure["ìŠ¤í¬í¬"][:20],
            "2ë‹¨ê³„_ì„œë¸ŒìŠ¤í¬í¬": structure["ì„œë¸ŒìŠ¤í¬í¬"][:30],
            "3ë‹¨ê³„_ìŠˆí¼ë¡±í…Œì¼": structure["ìŠˆí¼ë¡±í…Œì¼"][:50]
        },
        "ì „ì²´_í‚¤ì›Œë“œ": sorted(list(all_keywords))
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    return output_path


# === ë©”ì¸ ì‹¤í–‰ ===
def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python collect-longtail-keywords.py <í‚¤ì›Œë“œ> [--depth N]")
        print("ì˜ˆì‹œ: python collect-longtail-keywords.py í‡´ì§ê¸ˆ")
        print("      python collect-longtail-keywords.py ì—°ë§ì •ì‚° --depth 3")
        sys.exit(1)

    seed_keyword = sys.argv[1]

    # ê¹Šì´ ì˜µì…˜ íŒŒì‹±
    max_depth = 2  # ê¸°ë³¸ê°’
    if "--depth" in sys.argv:
        idx = sys.argv.index("--depth")
        if idx + 1 < len(sys.argv):
            max_depth = int(sys.argv[idx + 1])

    print("=" * 60)
    print(f"ğŸš€ ìŠˆí¼ ë¡±í…Œì¼ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸° v1.0")
    print(f"ğŸ“Œ ì‹œë“œ í‚¤ì›Œë“œ: {seed_keyword}")
    print(f"ğŸ“Š í™•ì¥ ê¹Šì´: {max_depth}")
    print("=" * 60)

    # í‚¤ì›Œë“œ ìˆ˜ì§‘
    all_keywords, level_keywords = collect_with_tail_biting(
        seed_keyword,
        max_depth=max_depth
    )

    # êµ¬ì¡° ë¶„ë¥˜
    structure = classify_structure(seed_keyword, level_keywords)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"ğŸ‘‘ ìµœì¢… ê²°ê³¼")
    print(f"ğŸ¯ ê²€ìƒ‰ì–´: {seed_keyword}")
    print(f"ğŸ“Š ì´ ê°œìˆ˜: {len(all_keywords)}ê°œ")
    print("=" * 60)

    print(f"\nğŸ“ êµ¬ì¡° ë¶„ì„:")
    print(f"  - í—ˆë¸Œ: {structure['í—ˆë¸Œ']}")
    print(f"  - ìŠ¤í¬í¬: {len(structure['ìŠ¤í¬í¬'])}ê°œ")
    print(f"  - ì„œë¸ŒìŠ¤í¬í¬: {len(structure['ì„œë¸ŒìŠ¤í¬í¬'])}ê°œ")
    print(f"  - ìŠˆí¼ë¡±í…Œì¼: {len(structure['ìŠˆí¼ë¡±í…Œì¼'])}ê°œ")

    print(f"\nğŸ” ìŠ¤í¬í¬ TOP 10:")
    for i, kw in enumerate(structure['ìŠ¤í¬í¬'][:10], 1):
        print(f"  {i}. {kw}")

    print(f"\nğŸ” ì„œë¸ŒìŠ¤í¬í¬ TOP 10:")
    for i, kw in enumerate(structure['ì„œë¸ŒìŠ¤í¬í¬'][:10], 1):
        print(f"  {i}. {kw}")

    print(f"\nğŸ” ìŠˆí¼ë¡±í…Œì¼ TOP 10:")
    for i, kw in enumerate(structure['ìŠˆí¼ë¡±í…Œì¼'][:10], 1):
        print(f"  {i}. {kw}")

    # ì €ì¥
    output_path = save_results(seed_keyword, all_keywords, structure)

    print(f"\nâœ¨ ì™„ë£Œ! {output_path}ì—ì„œ ì „ì²´ í‚¤ì›Œë“œ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
