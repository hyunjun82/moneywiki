#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì–‘ì‹ ê²€ìƒ‰ + ë‹¤ìš´ë¡œë“œ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

ë°©ì‹:
1. ì •ë¶€ê¸°ê´€ ì‚¬ì´íŠ¸ ì§ì ‘ ê²€ìƒ‰ (ì •ë¶€24, ë²•ì œì²˜, êµ­ì„¸ì²­ ë“±)
2. HWP/PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ
3. ìë™ ë‹¤ìš´ë¡œë“œ

ì‚¬ìš©ë²•:
    python search-and-download-forms.py íì—…ì‹ ê³ ì„œ
    python search-and-download-forms.py --all
    python search-and-download-forms.py --list
"""

import os
import sys
import re
import json
import time
import argparse
from pathlib import Path
from urllib.parse import urljoin, quote, unquote, urlparse

import requests
from bs4 import BeautifulSoup

# ============================================================
# ì„¤ì •
# ============================================================
PROJECT_ROOT = Path(__file__).parent.parent
FORMS_DIR = PROJECT_ROOT / "public" / "files" / "forms"
DATA_DIR = PROJECT_ROOT / "data" / "forms"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

# ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¥ì
DOWNLOAD_EXTENSIONS = ['.hwp', '.pdf', '.doc', '.docx', '.xls', '.xlsx']


# ============================================================
# ì•Œë ¤ì§„ ë‹¤ìš´ë¡œë“œ URL ë§¤í•‘ (ì§ì ‘ í™•ì¸ëœ URL)
# ============================================================
KNOWN_DOWNLOAD_URLS = {
    # êµ­ì„¸ì²­
    "íì—…ì‹ ê³ ì„œ": "https://www.nts.go.kr/comm/ntsFileDown.do?filePath=/upload/nts/03/0301/_info_%ED%86%B5%ED%95%A9%ED%8F%90%EC%97%85%EC%8B%A0%EA%B3%A0%EC%84%9C.hwp",
    "ì‚¬ì—…ìë“±ë¡ì‹ ì²­ì„œ": "https://www.nts.go.kr/comm/ntsFileDown.do?filePath=/upload/nts/03/0301/_info_%EC%82%AC%EC%97%85%EC%9E%90%EB%93%B1%EB%A1%9D%EC%8B%A0%EC%B2%AD%EC%84%9C.hwp",
    "íœ´ì—…ì‹ ê³ ì„œ": "https://www.nts.go.kr/comm/ntsFileDown.do?filePath=/upload/nts/03/0301/_info_%ED%9C%B4%EC%97%85%EC%8B%A0%EA%B3%A0%EC%84%9C.hwp",

    # ê°•ë‚¨êµ¬ì²­ (ê°€ì¡±ê´€ê³„)
    "í˜¼ì¸ì‹ ê³ ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010099/download.do",
    "ì¶œìƒì‹ ê³ ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010098/download.do",
    "ì‚¬ë§ì‹ ê³ ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010100/download.do",
    "ì´í˜¼ì‹ ê³ ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010101/download.do",

    # ê³ ìš©ë…¸ë™ë¶€ (7ì¢… ê·¼ë¡œê³„ì•½ì„œ)
    "í‘œì¤€ê·¼ë¡œê³„ì•½ì„œ": "https://www.moel.go.kr/common/downloadFile.do?file_seq=20190700012&bbs_seq=20190700008&bbs_id=29&file_ext=hwp",

    # ì •ë¶€24/ë²•ì œì²˜
    "ì „ì…ì‹ ê³ ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010107/download.do",
    "ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì‹ ì²­ì„œ": "https://www.gangnam.go.kr/file/1/get/FILE_000000000010108/download.do",
}


# ============================================================
# ì •ë¶€24 ê²€ìƒ‰
# ============================================================
def search_gov24(form_name):
    """
    ì •ë¶€24ì—ì„œ ë¯¼ì›ì„œì‹ ê²€ìƒ‰
    """
    print(f"  ğŸ” ì •ë¶€24 ê²€ìƒ‰: {form_name}")

    search_url = f"https://www.gov.kr/search/applyMw?query={quote(form_name)}"

    try:
        response = requests.get(search_url, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, 'lxml')

        results = []
        for link in soup.select('a[href*="CappBizCD"]'):
            title = link.get_text(strip=True)
            href = link.get('href', '')
            if form_name in title and href:
                results.append({
                    'title': title,
                    'url': urljoin("https://www.gov.kr", href)
                })

        return results[:5]

    except Exception as e:
        print(f"    âŒ ì •ë¶€24 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# ê°•ë‚¨êµ¬ì²­ ê²€ìƒ‰ (ë¯¼ì›ì„œì‹ ì˜ ì •ë¦¬ë¨)
# ============================================================
def search_gangnam(form_name):
    """
    ê°•ë‚¨êµ¬ì²­ ë¯¼ì›ì„œì‹ì—ì„œ ê²€ìƒ‰
    """
    print(f"  ğŸ” ê°•ë‚¨êµ¬ì²­ ê²€ìƒ‰: {form_name}")

    # ë¯¼ì›ì„œì‹ ëª©ë¡ í˜ì´ì§€
    base_url = "https://www.gangnam.go.kr/board/B_000060/list.do"

    try:
        params = {'searchKeyword': form_name}
        response = requests.get(base_url, headers=HEADERS, params=params, timeout=30)
        soup = BeautifulSoup(response.text, 'lxml')

        results = []
        for link in soup.select('a[href*="view.do"]'):
            title = link.get_text(strip=True)
            href = link.get('href', '')
            if href:
                results.append({
                    'title': title,
                    'url': urljoin("https://www.gangnam.go.kr", href)
                })

        return results[:10]

    except Exception as e:
        print(f"    âŒ ê°•ë‚¨êµ¬ì²­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ
# ============================================================
def find_download_links(url, form_name):
    """
    í˜ì´ì§€ì—ì„œ HWP/PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
    """
    print(f"    ğŸ“„ í˜ì´ì§€ ë¶„ì„ ì¤‘...")

    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'lxml')

        download_links = []

        # ëª¨ë“  ë§í¬ ê²€ì‚¬
        for link in soup.select('a[href]'):
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # ë‹¤ìš´ë¡œë“œ ë§í¬ íŒë³„
            is_download = False
            file_ext = None

            # 1. í™•ì¥ìë¡œ íŒë³„
            for ext in DOWNLOAD_EXTENSIONS:
                if ext in href.lower():
                    is_download = True
                    file_ext = ext.replace('.', '')
                    break

            # 2. download í‚¤ì›Œë“œë¡œ íŒë³„
            if any(kw in href.lower() for kw in ['download', 'filedown', 'ntsfiledown', 'file/1/get']):
                is_download = True
                # í™•ì¥ì ì¶”ì¸¡
                if 'hwp' in href.lower() or 'hwp' in text.lower() or '.hwp' in text.lower():
                    file_ext = 'hwp'
                elif 'pdf' in href.lower() or 'pdf' in text.lower():
                    file_ext = 'pdf'
                elif 'doc' in href.lower() or 'doc' in text.lower():
                    file_ext = 'docx'
                else:
                    file_ext = 'hwp'  # ê¸°ë³¸ê°’

            if is_download and file_ext:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(url, href)

                # ê´€ë ¨ì„± ì ìˆ˜
                relevance = 0
                if form_name in text or form_name in unquote(href):
                    relevance = 100
                elif any(kw in text for kw in ['ì–‘ì‹', 'ì„œì‹', form_name[:2]]):
                    relevance = 50
                else:
                    relevance = 10

                download_links.append({
                    'url': full_url,
                    'text': text[:50] if text else 'ë‹¤ìš´ë¡œë“œ',
                    'ext': file_ext,
                    'relevance': relevance
                })

        # ê´€ë ¨ì„± ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        download_links.sort(key=lambda x: x['relevance'], reverse=True)

        return download_links[:5]

    except Exception as e:
        print(f"    âŒ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# íŒŒì¼ ë‹¤ìš´ë¡œë“œ
# ============================================================
def download_file(url, save_path):
    """
    íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=60, stream=True, allow_redirects=True)

        if response.status_code != 200:
            print(f"    âŒ HTTP {response.status_code}")
            return False

        # íŒŒì¼ í¬ê¸° ì²´í¬
        content_length = response.headers.get('Content-Length', 0)
        if content_length and int(content_length) < 500:
            print(f"    âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ ({content_length} bytes)")
            return False

        # HTML ì²´í¬
        content_type = response.headers.get('Content-Type', '')
        if 'text/html' in content_type and 'attachment' not in response.headers.get('Content-Disposition', ''):
            # ì²˜ìŒ 100ë°”ì´íŠ¸ í™•ì¸
            first_bytes = response.content[:100]
            if b'<!DOCTYPE' in first_bytes or b'<html' in first_bytes:
                print(f"    âš ï¸ HTML í˜ì´ì§€ì„ (íŒŒì¼ ì•„ë‹˜)")
                return False

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, 'wb') as f:
            f.write(response.content)

        file_size = os.path.getsize(save_path)

        # ìµœì¢… í™•ì¸: HWP íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ ì²´í¬
        with open(save_path, 'rb') as f:
            header = f.read(8)

        # HWP íŒŒì¼ ì‹œê·¸ë‹ˆì²˜: D0 CF 11 E0 (OLE compound)
        if save_path.endswith('.hwp') and not header.startswith(b'\xd0\xcf\x11\xe0'):
            print(f"    âš ï¸ ìœ íš¨í•œ HWP íŒŒì¼ì´ ì•„ë‹˜")
            os.remove(save_path)
            return False

        print(f"    âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {os.path.basename(save_path)} ({file_size:,} bytes)")
        return True

    except Exception as e:
        print(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# ============================================================
# JSON ì—…ë°ì´íŠ¸
# ============================================================
def update_json(form_name, ext, source_url=None):
    """
    ì–‘ì‹ JSON íŒŒì¼ ì—…ë°ì´íŠ¸
    """
    json_path = DATA_DIR / f"{form_name}.json"

    if not json_path.exists():
        print(f"    âš ï¸ JSON íŒŒì¼ ì—†ìŒ: {form_name}.json")
        return

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì¶”ê°€
        if 'downloads' not in data or not data['downloads']:
            data['downloads'] = {}
        if 'downloadNames' not in data or not data['downloadNames']:
            data['downloadNames'] = {}

        data['downloads'][ext] = f"/files/forms/{form_name}.{ext}"
        data['downloadNames'][ext] = f"{form_name}.{ext}"

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"    ğŸ“ JSON ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    except Exception as e:
        print(f"    âŒ JSON ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


# ============================================================
# ë©”ì¸ ì²˜ë¦¬
# ============================================================
def process_form(form_name):
    """
    ì–‘ì‹ ê²€ìƒ‰ â†’ ë‹¤ìš´ë¡œë“œ â†’ JSON ì—…ë°ì´íŠ¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {form_name}")
    print('='*60)

    # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëëŠ”ì§€ í™•ì¸
    for ext in ['hwp', 'pdf', 'docx']:
        existing = FORMS_DIR / f"{form_name}.{ext}"
        if existing.exists() and existing.stat().st_size > 1000:
            print(f"  âœ… ì´ë¯¸ ì¡´ì¬: {form_name}.{ext}")
            return True

    # 1. ì•Œë ¤ì§„ URL í™•ì¸
    if form_name in KNOWN_DOWNLOAD_URLS:
        print(f"  ğŸ“Œ ì•Œë ¤ì§„ URL ì‚¬ìš©")
        url = KNOWN_DOWNLOAD_URLS[form_name]

        # í™•ì¥ì ì¶”ì¶œ
        ext = 'hwp'
        if '.pdf' in url.lower():
            ext = 'pdf'
        elif '.doc' in url.lower():
            ext = 'docx'

        save_path = str(FORMS_DIR / f"{form_name}.{ext}")

        if download_file(url, save_path):
            update_json(form_name, ext)
            return True

    # 2. ê°•ë‚¨êµ¬ì²­ ê²€ìƒ‰
    results = search_gangnam(form_name)

    for result in results:
        if form_name in result['title'] or result['title'] in form_name:
            download_links = find_download_links(result['url'], form_name)

            for link in download_links:
                ext = link['ext']
                save_path = str(FORMS_DIR / f"{form_name}.{ext}")

                print(f"    â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì‹œë„: {link['text']}")

                if download_file(link['url'], save_path):
                    update_json(form_name, ext)
                    return True

    # 3. ì •ë¶€24 ê²€ìƒ‰
    results = search_gov24(form_name)

    for result in results:
        download_links = find_download_links(result['url'], form_name)

        for link in download_links:
            ext = link['ext']
            save_path = str(FORMS_DIR / f"{form_name}.{ext}")

            print(f"    â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì‹œë„: {link['text']}")

            if download_file(link['url'], save_path):
                update_json(form_name, ext)
                return True

    print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
    return False


def get_missing_forms():
    """
    ë‹¤ìš´ë¡œë“œ íŒŒì¼ì´ ì—†ëŠ” ì–‘ì‹ë§Œ ê°€ì ¸ì˜¤ê¸°
    """
    missing = []
    for json_file in DATA_DIR.glob('*.json'):
        form_name = json_file.stem
        has_file = any((FORMS_DIR / f"{form_name}.{ext}").exists()
                       for ext in ['hwp', 'pdf', 'docx', 'doc', 'xlsx', 'xls'])
        if not has_file:
            missing.append(form_name)
    return sorted(missing)


def main():
    parser = argparse.ArgumentParser(description='ì •ë¶€ê¸°ê´€ ì–‘ì‹ ê²€ìƒ‰ & ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('form_name', nargs='?', help='ì–‘ì‹ëª…')
    parser.add_argument('--all', '-a', action='store_true', help='ë¯¸ë‹¤ìš´ë¡œë“œ ì–‘ì‹ ì „ì²´')
    parser.add_argument('--list', '-l', action='store_true', help='ëª©ë¡ í™•ì¸')
    parser.add_argument('--missing', '-m', action='store_true', help='ë¯¸ë‹¤ìš´ë¡œë“œ ëª©ë¡')

    args = parser.parse_args()

    FORMS_DIR.mkdir(parents=True, exist_ok=True)

    if args.list or args.missing:
        missing = get_missing_forms()
        downloaded = [f.stem for f in FORMS_DIR.glob('*.hwp')] + [f.stem for f in FORMS_DIR.glob('*.pdf')]
        print(f"\nğŸ“Š í˜„í™©: ë‹¤ìš´ë¡œë“œ {len(set(downloaded))}ê°œ / ë¯¸ë‹¤ìš´ë¡œë“œ {len(missing)}ê°œ")
        print(f"\nâ¬œ ë¯¸ë‹¤ìš´ë¡œë“œ ({len(missing)}ê°œ):")
        for form in missing[:20]:
            print(f"   {form}")
        if len(missing) > 20:
            print(f"   ... ì™¸ {len(missing)-20}ê°œ")
        return

    if args.all:
        missing = get_missing_forms()
        print(f"\nğŸš€ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ({len(missing)}ê°œ)")

        success = 0
        fail = 0

        for form in missing:
            if process_form(form):
                success += 1
            else:
                fail += 1
            time.sleep(1)

        print(f"\n{'='*60}")
        print(f"ğŸ“Š ê²°ê³¼: ì„±ê³µ {success}ê°œ, ì‹¤íŒ¨ {fail}ê°œ")
        return

    if args.form_name:
        process_form(args.form_name)
        return

    print("\nì‚¬ìš©ë²•:")
    print("  python search-and-download-forms.py íì—…ì‹ ê³ ì„œ    # íŠ¹ì • ì–‘ì‹")
    print("  python search-and-download-forms.py --all        # ì „ì²´ ë‹¤ìš´ë¡œë“œ")
    print("  python search-and-download-forms.py --missing    # ë¯¸ë‹¤ìš´ë¡œë“œ ëª©ë¡")


if __name__ == "__main__":
    main()
